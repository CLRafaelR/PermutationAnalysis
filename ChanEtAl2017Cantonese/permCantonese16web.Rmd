---
title: "Cantonese"
output:
  html_document: 
    fig_width: 9
  word_document: default
---

This is final permutation and mixed model analysis for Chan, Yang, Chang, & Kidd (in press)

```{r require-packages}
knitr::opts_chunk$set(warning = FALSE)
knitr::knit_theme$set("zmrok")

## The following loads ggplot2 and stringr
require(tidyverse)

##---------------------------------------------------
##
## Load pipes
## In this document, the native pipe `|>`,
## which is from base, is usually used
## to replace magrittr's `%>%` with,
## since `|>`'s process is faster than `%>%`'s.
## 
## However, magrittr is necessary since it provides
## far more varieties of functions, such as
## the exposition pipe `%$% `
## the assign pipe `%<>% `
## the Tee pipe `%T>% `
##---------------------------------------------------
require(magrittr)

require(stringr)
require(lme4)
require(nlme)      ## for lme()
require(lsmeans)   ## for lsmeans()
require(multcomp)  ## for multiple comparison stuff
require(remef)
print(R.version.string)
print(sessionInfo())

apaformat <- function(){
  # make the theme black-and-white
  # rather than grey
  # (do this before font changes, or it overrides them)
  theme_bw() +
  theme(
    # switch off major gridlines
    panel.grid.major = element_blank(),

    # switch off minor gridlines
    panel.grid.minor = element_blank(),

    # switch off the rectangle around symbols in the legend
    legend.key = element_blank()
  )
}
```

In the following code chunks,
the combination of `tidyverse` functions and pipe (`|>`), rather than base functions, are applied.
Such 'tidy' methods will enhance the readability of the codes
and speed of the code evaluation.
Moreover, the tidy methods enable you to save memory,
since you do not have to create intermediate objects.

Although the application of the tidy methods may change the row order in a data frame,
the row values are identical.
You can check this by runnning:

```
dplyr::all_equal(
  data.frame.created.by.base.function,
  corresponding.data.frame.created.by.tidy.methods,
  ignore_col_order = FALSE,
  ignore_row_order = FALSE,
  convert = TRUE
  )
```

```{r data-preparation}
# Create a data frame with onset-offset data for each condition in study
onsetStimCL = data.frame(
  SentenceType = c(
    rep("Subject", 3),
    rep("Object", 4)
    ),
  cat = c(
    "VN", "D CL", "Head",
    "N", "V", "D CL", "Head"
    ),
  onset = c(
    0, 0.625, 0.91,
    0, 0.412, 0.661, 0.925
    ),
  HeadNoun = rep("CL", 7)
  )

onsetStimGE = data.frame(
  SentenceType = c(
    rep("Subject", 3),
    rep("Object", 4)
    ),
  cat = c(
    "VN", "ge", "Head",
    "N", "V", "ge", "Head"
    ),
  onset = c(
    0, 0.741, 0.899,
    0, 0.488, 0.756, 0.939
    ),
  HeadNoun = rep("ge3", 7)
  )

onsetStim = bind_rows(
  onsetStimCL,
  onsetStimGE
  ) |>
  mutate(
    time = onset * 1000,
    time2 = c(
      622, 907, 1497, 409,
      658, 922, 1387, 738,
      896, 1454, 485, 753,
      936, 1477
      ),
    ypos = rep(
      c(rep(0.95, 3), rep(0.75, 4)),
      2
      ),
    target = 1,
    restarg = 0
  )


##  mutate(
##    time = onset * 1000,
##    time2 = case_when(
##        cat == "Head" & SentenceType == "Subject" & HeadNoun == "CL"
##          ~ 1.477*1000,
##        cat == "Head" & SentenceType == "Object"  & HeadNoun == "CL"
##          ~ 1.387*1000,
##        cat == "Head" & SentenceType == "Subject" & HeadNoun == "ge3"
##          ~ 1.454*1000,
##        cat == "Head" & SentenceType == "Object"  & HeadNoun == "ge3"
##          ~ 1.477*1000,
##        TRUE ~ lag(
##          time, 0
##          )
##        )
##    )

onsetStim |>
  filter(cat == "Head")

onsetStim |>
  group_by(
    HeadNoun, cat
  ) |>
  summarise(
    time2 = mean(time2)
  ) |>
  ungroup()
```

```{r data-eye-movement}
# load eye-tracking data
# with `reader::read_csv` (from tidyverse)
#
# `reader::read_csv` interprets the data with
# greater acurracy than `base::read.csv` and
# `reader::read_csv` prunes the following line:
# bothdata$time = as.numeric(as.character(bothdata$time))
bothdata = read_csv("CanRCeyetracking data-19 for CL &18 for GE-only TTR longer than 200ms.csv")  |>
  mutate(
    HeadNoun = factor(
      HeadNoun,
      labels = c("CL", "ge3")
      ),
    SentenceType = factor(
      SentenceType,
      labels = c("Subject", "Object"),
      levels = c("subject", "object")
      ),
    Item = str_replace(
      Item,
      "(S|O)",
      "I"
      ),
  time = time * 1000,
  target = `T`
  )

xtabs(~ Item + HeadNoun, bothdata)
xtabs(~ Item + SentenceType, bothdata)
```

1. Group `bothdata` by the columns `time`, `HeadNoun`, and `SentenceType` using `dplyr::group_by()`
2. Calculate the mean proportion of gaze to `target` by the group created using `dplyr::group_by(time, HeadNoun, SentenceType)`
3. Ungroup (Remove grouping)
4. Plot the mean proportion of gaze to `target` using `ggplot2::ggplot()`

```{r figure-eye-movement}
# Figure to see raw data
bothdata |>
  group_by(time, HeadNoun, SentenceType) |>
  summarise(target = mean(target)) |>
  ungroup() |>
  ggplot(
    aes(
      x = time,
      y = target,
      colour=SentenceType
      )
    ) +
  geom_line() +
  scale_colour_brewer(palette="Set1") +
  # change background to white
  theme_bw() +
  ylab("Target Proportions") +
  #ylim(0,1) +
  facet_wrap(~ HeadNoun, ncol=1) +
  geom_rect(
    data = onsetStim,
    aes(
      xmin = time,
      xmax = time2-33,
      ymin = ypos-0.13,
      ymax = ypos,
      colour = SentenceType
      ),
    fill = NA,
    show.legend = FALSE
    ) +
  geom_text(
    data = onsetStim,
    aes(
      x = (time+time2)/2,
      y = ypos-0.05,
      label = cat
      ),
    hjust = 0.7,
    size = 3,
    show.legend = FALSE
  )
```

Above is the raw data, below we remove the mean of time 0.

1. Assign the following data to the object named `both`
2. From `bothdata`, `filter` out the data whose `time` is 0 
3. Group that data by the columns `Participant`, `Item`, `HeadNoun`, and `SentenceType` using `dplyr::group_by()`
4. Calculate the mean proportion of gaze to `target` by the group created using `dplyr::group_by(Participant, Item, HeadNoun, SentenceType)`
5. Ungroup
6. `Join` that filtered data to the original `bothdata` by the columns `Participant`, `Item`, `HeadNoun`, and `SentenceType`; at the same time, attach `suffix`es to the duplicate columns (especially the column `target`), so that you can identify which data frame the duplicate columns originally came from.
7. `Mutate` the joined data, so that its `target` indicates the subtraction of `target.filtered` from `target.original`

```{r preparation-both-data}
both <- bothdata |>
  filter(time == 0) |>
  group_by(Participant, Item, HeadNoun, SentenceType) |>
  summarise(target = mean(target)) |>
  ungroup() |>
  right_join(
    # filtered `bothdata` is the first data frame to be combined
    #`bothdata` is the second data frame to be combined
    bothdata,
    by = c(
      "Participant",
      "Item",
      "HeadNoun",
      "SentenceType"
      ),
    # Suffixes must be in this order,
    # since filtered `bothdata` is the first data frame and
    # original `bothdata` is the second:
    suffix = c(".filtered", ".original")
  ) |>
  mutate(
    target = target.original - target.filtered
  )
```


```{r figure-both}
# plot figure for dataset
both |>
  group_by(
    time, HeadNoun, SentenceType
  ) |> 
  summarise(
    target = mean(target)
  ) |>
  ungroup() |>
  ggplot(
    aes(
      x = time,
      y = target,
      colour = SentenceType
      )
    ) +
  geom_line() +
  scale_colour_brewer(palette = "Set1") +
  # change background to white
  theme_bw() +
  ylab("Looks to target") +
  #ylim(0,1) +
  facet_wrap(~ HeadNoun, ncol = 1) +
  geom_rect(
    data = onsetStim,
    aes(
      xmin = time,
      xmax = time2 - 33,
      ymin = ypos - 0.13,
      ymax = ypos,
      colour = SentenceType
      ),
    fill = NA,
    show.legend = FALSE
    ) +
    geom_text(
      data = onsetStim,
      aes(
        x = (time + time2)/2,
        y = ypos - 0.05,
        label = cat),
        size = 3,
        show.legend = FALSE
    )
```

Here is a mixed model analysis using 200 ms windows

```{r}
winsize = 200

#unique(both$time)

both <- both |> 
  mutate(
    win = as.integer((time+10)/winsize)
  )

xtabs(~ win, both)

# not enough data in 12
both2 <- both |>
  filter(
    win != 12,
    !is.na(time)
  )

xtabs(~ win, both2)

xtabs(~ Item + HeadNoun, both)

xtabs(~ Participant + SentenceType, both)

subjmeans.df <- both |>
  group_by(
    win, HeadNoun, SentenceType, Participant, Item
  ) |>
  summarise(
    target = mean(target)
  ) |>
  ungroup() |>
  mutate(
    time = win * winsize,
    cwin = win - mean(win),#scale(win, scale = FALSE),
    cCL = if_else(
      HeadNoun == "CL",
      0.5,
      -0.5
      ),
    cobject = if_else(
      SentenceType == "Object",
      0.5,
      -0.5
      ),
    Participant = factor(Participant),
    Item = factor(Item)
  )
```

```{r lmer-all-window}
mixmodel = lmer(
  target ~ cwin*cCL*cobject +
    (1 + SentenceType | Participant) +
    (1+SentenceType | Item),
  subjmeans.df
  )

mixmodel |>
  summary() |>
  print()
```

```{r lmer-8th-window}
subjmeans.df |>
  filter(
    win == 8,
    HeadNoun == "CL"
  ) |>
  #### lmer()
  (
    \(x) lmer(
      target ~ cobject + 
        (1+SentenceType | Participant) +
        (1+SentenceType | Item),
      data = x
      )
  )() |>
    summary() |>
    print()
```

```{r cor-9th-10th-pipe-1}
subjmeans.df |>
  filter(
    win == 9,
    HeadNoun == "ge3"
  ) |>
  dplyr::select(
    target
  ) |>
  cor(
    subjmeans.df |>
      filter(
        win == 10,
        HeadNoun == "ge3"
      ) |>
    dplyr::select(
      target
    )
  )
```

```{r cor-9th-10th-pipe-2}
subjmeans.df |>
  (\(x) {
    filter(
      x,
      win == 9,
      HeadNoun == "ge3"
      ) |>
      dplyr::select(target) ->
      Ge3.9th

    filter(
      x,
      win == 10,
      HeadNoun == "ge3"
      ) |>
      dplyr::select(target) ->
      Ge3.10th

    list(Ge3.9th, Ge3.10th)
  })() |>
  (
    \(x){
      cor(x[[1]], x[[2]])
      }
  )()
```

```{r cor-9th-10th-pipe-3}
subjmeans.df |>
  (\(x){
    list(
      filter(
        x,
        win == 9,
        HeadNoun == "ge3"
        ) |>
        dplyr::select(target),
      filter(
        x,
        win == 10,
        HeadNoun == "ge3"
        ) |>
        dplyr::select(target)
    )
  })() |>
  (
    \(x){
      cor(x[[1]], x[[2]])
      }
  )()
```

Since we have a significant three way interaction of HeadNoun, SentenceType, and window, we can do posthocs.
We use lsmeans to do contrasts between obj and sub for each window in each HeadNoun.
lsmeans also adjusts p automatically for the number of comparisons.

```{r coding-setting}
#options("contrasts")

# Dummy coding (`contr.treatment`) for an unordered factor
# Orthogonal polynomial coding (`contr.poly`) for an ordered factor
options(
  contrasts = c(
    "contr.treatment",
    "contr.poly"
    )
  )
```

```{r data-for-posthoc}
subjmeans.df <- subjmeans.df |>
  mutate(
    win = as_factor(win),
    HeadNoun = factor(HeadNoun),
    SentenceType = factor(SentenceType)
  )
```

```{r}
mixmodelFactor = lmer(
  target ~ win * SentenceType * HeadNoun +
  (1 + SentenceType | Participant) +
  (1 + SentenceType | Item),
  subjmeans.df
  )

posthocs <- lsmeans(
  mixmodelFactor,
  ~ SentenceType | win * HeadNoun
  ) |>
  pairs() |>
  as.glht() |>
  summary()
```

```{r }
subjmeans.df <- subjmeans.df |>
  mutate(
    pred = remef(mixmodelFactor, ran = "all")
  )

timelist <- subjmeans.df$time |>
  unique() |>
  as.integer()

meansigPost <- tibble(
  time = rep(timelist, 2),
  HeadNoun = rep(c("CL", "ge3"), each = length(timelist)),
  pval = as.numeric(
    purrr::map(
      posthocs,
      ~ { return(.x$test$pvalues[1]) }
      )
    ),
  target = 0,
  pred = 0,
  SentenceType = "Object"
  ) |>
  filter(pval < 0.05)
```

```{r figure}
subjmeans.df |>
  group_by(
    time, HeadNoun, SentenceType
  ) |>
  summarise(
    pred = mean(pred),
    target = mean(target)
  ) |>
  ungroup() |>
  ggplot(
    aes(
      x = time,
      y = target,
      colour = SentenceType
      )
    ) +
  # color them grey
  geom_rect(
    data = meansigPost,
    aes(
      xmin = time,
      xmax = time + winsize,
      ymin = -Inf,
      ymax = Inf
      ),
      colour = NA,
      fill = "grey90",
      show.legend = FALSE
      ) +
  geom_line() +
  facet_wrap(
    ~ HeadNoun,
    ncol = 1
    ) +
  scale_colour_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set2") +
  geom_vline(
    xintercept = seq(0, 2600, by = winsize),
    colour = "black",
    linetype = "dashed" #2
    ) +
  apaformat()
```

This is the permutation analysis

```{r}
# create copy of data frame
wdivsize = 1
pdata <- both |>
  filter(!is.na(time)) |>
  mutate(
    cobject = if_else(SentenceType == "Subject", 0.5, -0.5),
    win = as.integer(time/wdivsize),
    time = win * wdivsize
  )

# create data frame which averages over subjects.
# This also stores the results of the permutation analysis
means.df <- pdata |>
  group_by(
    SentenceType, cobject, time, HeadNoun
  ) |>
  summarise(
    target = mean(target)
  ) |>
  ungroup() |>
  mutate(pstr = 1000)


# We do this for each timebin in the data
timelist = unique(pdata$time)
for (l in c("CL", "ge3")){
  for (t in timelist){
    # create data frame for ONE timebin for each HeadNoun level
    onetime = subset(pdata,time == t & HeadNoun == l)
    # do regression model on target using structure SentenceTypeition
    onemodel = lm(target ~ cobject, onetime)
    # print(summary(tarmodel))
    # this is the t-value for structure
    targetT = coef(summary(onemodel))[2,3]  # observed t-value
    targetP = abs(coef(summary(onemodel))[2,4])  # observed p-value
    means.df$tstr[means.df$time == t & means.df$HeadNoun == l] = targetT
    means.df$pstr[means.df$time == t & means.df$HeadNoun == l] = targetP
  }
}

# to see these p-values, we draw them arbitrarily on the graph at 0.2.
# when the p-value < 0.05, we draw a blue line above 0.2
# when the p-value > 0.05, we draw an orange line below 0.2
pliney = -0.1
plinemax = 0.2
means.df$pline = pliney+plinemax*(0.05-means.df$pstr)
means.df$plinecol = ifelse(means.df$pstr < 0.05,"a","b")
wsize = mean(pdata$time[2:5] - pdata$time[1:4] - 8)/2
means.df$SentenceType=factor(means.df$SentenceType,levels=c("Subject","Object"))
p = ggplot(means.df , aes( x = time, y = target, colour=SentenceType))
p = p + geom_line()
p = p + scale_colour_brewer(palette="Set1")
p = p + scale_fill_brewer(palette="Set2")
p = p + theme_bw()   # change background to white
p = p + facet_wrap(~ HeadNoun, ncol=1)
p = p + geom_rect(aes(xmin=time-wsize, xmax=time+wsize, ymin = pliney, ymax= pline, fill=plinecol),colour=NA,show.legend=FALSE)
p
```

```{r}
# Also, each window is not independent, so we create clusters for adjacent windows with p<0.05
# cnum is the cluster number and we increment the number when the p value is > 0.05
# so clusters with the same cnum are part of the same cluster
cnum = 1
lastpval = 100
lasttdir = 1
means.df$cnum = 1
for (l in c("CL","ge3")){
  for (t in timelist[2:length(timelist)]){
    onetime = subset(means.df,time == t & HeadNoun == l & SentenceType == "Object")
    pval = abs(onetime$pstr)
    tdir = onetime$tstr
    if (pval < 0.05 & lastpval > 0.05 ){
      cnum = cnum + 1  # increase cluster number when entering a significant cluster from a non-significant cluster
    }
    if (pval > 0.05 ){  
      cnum = cnum + 1 # increase cluster number when not significant
    }else{
      # if t value flips direction, even if both are signif, 
      # we should treat those as separate clusters
      if (lasttdir*tdir < 0){
        cnum = cnum + 1 
      }
    }
    lastpval = pval
    lasttdir = tdir
    means.df$cnum[means.df$time == t & means.df$HeadNoun == l] = cnum
  }
  cnum=cnum+1  # new cluster for different HeadNounuages
}
head(means.df,10)

# this shows the clusters
p = ggplot(means.df , aes( x = time, y = target, colour=SentenceType, label=cnum))
p = p + scale_colour_brewer(palette="Set1")
p = p + scale_fill_brewer(palette="Set2")
p = p + theme_bw()   # change background to white
p = p + ylab("Looks to passive match")
#p = p + ylim(0,1)
p = p + facet_wrap(~ HeadNoun, ncol=1)
p = p + geom_rect(aes(xmin=time-wsize, xmax=time+wsize, ymin = pliney, ymax= pline, fill=plinecol),colour=NA,show.legend=FALSE)
p + geom_text(size=2)
```


```{r}
# we now want to identify the clusters that were significant
# p-values are same for subject/object, so just use object.
meansonlyact.df = subset(means.df, SentenceType == "Object")
sigcluster = subset(meansonlyact.df, abs(pstr) < 0.05 )
print(sigcluster,digits=3)
# this computes the sum of the t-values for each cluster
sumcluster  = aggregate(tstr ~ cnum + HeadNoun, meansonlyact.df, sum)
head(sumcluster)

# here are the start and finish times for significant clusters
timedf = aggregate(time ~ cnum + HeadNoun,sigcluster,min)
colnames(timedf)<-c("cnum","HeadNoun","starttime")
timedf2 = aggregate(time ~ cnum + HeadNoun,sigcluster,max)
timedf$endtime = timedf2$time+33
print(timedf)
paste(timedf$starttime,"-",timedf$endtime,"ms",sep="",collapse=",")
```


```{r}
# now we create a distribution of t-values (save in permdist)
# by randomly scrambling the subject/object labels for each time bin 1000 times
createPermDist <- function(filename="permDist.RData"){
  n = 1000
  exptests = data.frame()
  for (s in 1:length(sigcluster$time)){
    #  print(cl)
    cl = sigcluster$cnum[s] # cluster number
    b = sigcluster$time[s] # time
    l = sigcluster$HeadNoun[s] # HeadNounuage
    print(paste("b ",b," HeadNoun",l))
    # one time point
    onetime = subset(pdata,HeadNoun == l & time %in% b)
    # randSet is a copy of onetime dataframe that is scrambled
    randSet = onetime
    
    for (i in 1:n){
      #  set.seed(i)
      # randomly scramble cobject labels without replacement
      randSet$cobject = sample(randSet$cobject,length(randSet$cobject))
      # test if target is related to random scrambled cobject
      randmodel = lm(target ~ cobject ,randSet)
      #  print(summary(randmodel))
      # extract and save t-values
      t = coef(summary(randmodel))[2,3]
      df = data.frame(t=t,cluster=cl,time=b,HeadNoun=l,sim=i)
      exptests = rbind(exptests, df )
    }
  }
  save(exptests,file=filename)
  return(exptests)
}
# Each time you run the permutation test, you get different results
# to get the results in the paper, we use our previously saved permutation test
#load("permDistCan8.RData")

# If you want to run your own permutation test, you can uncomment this command and it will create the exptests data frame and save it in RData file.
exptests = createPermDist("permDistCan8.RData") 

# we sum over clusters so that longer clusters have stronger t-values
sumt.df =  aggregate(t ~ HeadNoun + cluster + sim, exptests, sum)
head(sumt.df)

# simulated sum cluster histogram
p = ggplot(sumt.df,aes(x = t))
p = p +geom_histogram(binwidth=0.5)
p = p + facet_wrap(~ HeadNoun)
p+theme_bw()

```


```{r}
# this code extracts out the maximum sum t for each simulation at each age
maxclusterdist = data.frame()
for (l in unique(sumt.df$HeadNoun)){
  for (s in unique(sumt.df$sim)) {
    # get all results for one simulation in one HeadNounuage
    onesim = subset(sumt.df,sim == s & HeadNoun == l)
    onesim$astruct = abs(onesim$t)
    # find max t-value
    maxrow = onesim[order(onesim$astruct,decreasing = T),]
    maxclusterdist = rbind(maxclusterdist,maxrow[1,])
  }
}
head(maxclusterdist)

# Shows the simulated distribution with maximum cluster t values
maxclusterdist2 = maxclusterdist[order(maxclusterdist$HeadNoun,maxclusterdist$t),]
p = ggplot(maxclusterdist,aes(x = t))
p = p +geom_histogram(binwidth=0.5)
p = p + facet_wrap(~ HeadNoun)
p+theme_bw()

maxclusterdist$type = "Maximal sum t"
sumt.df$astruct = abs(sumt.df$t)
sumt.df$type = "All sum t"
bothdist = rbind(sumt.df,maxclusterdist)
bothdist$c= 1
tvaldf = aggregate(c ~ type + HeadNoun, bothdist,sum)
tvaldf$upper= as.integer(tvaldf$c*0.975)
tvaldf$lower= as.integer(tvaldf$c*0.025)
sortbothdist = bothdist[order(bothdist$type,bothdist$HeadNoun,bothdist$t),]
tvaldf2 = tvaldf
for (e in tvaldf$HeadNoun){
  for (t in tvaldf$type){
    d = sortbothdist[sortbothdist$HeadNoun == e & sortbothdist$type==t,]
    u = tvaldf$upper[tvaldf$type == t & tvaldf$HeadNoun==e]
     l = tvaldf$lower[tvaldf$type == t & tvaldf$HeadNoun==e]
   tvaldf$bar[tvaldf$type == t & tvaldf$HeadNoun==e] = d$t[u]
    tvaldf2$bar[tvaldf2$type == t & tvaldf2$HeadNoun==e] = d$t[l]
 }
}
bothtvaldf = rbind(tvaldf2,tvaldf)

p = ggplot(bothdist,aes(x = t))
p = p +geom_histogram(binwidth=0.5)
#p = p +geom_vline(end,mapping=aes(xintercept=xint))
p = p + facet_wrap(~ type + HeadNoun, ncol=2)
p=p+theme_bw()+geom_vline(data=bothtvaldf,aes(xintercept=bar),linetype="dashed",size=1)
apaformat(p)+ylab("Number of t values")+xlab("t-values")
ggsave("dist.png",width=6,height=4)
```


```{r}
# here we test all significant clusters against corresponding distribution
clmaxclusterdist = subset(maxclusterdist,HeadNoun == "CL")
gemaxclusterdist = subset(maxclusterdist,HeadNoun == "ge3")
# this identifies cluster tvalues that are greater than dist t-values
for (cl in unique(sumcluster$cnum)){
  bins = unique(means.df[means.df$cnum == cl,]$time)
  lan = sumcluster$HeadNoun[sumcluster$cnum == cl][1]
  
  tstr = abs(sumcluster$tstr[sumcluster$cnum==cl])
  # use appropriate distribution
   if (lan == "CL"){
     permtdist = clmaxclusterdist$astruct
   }else{
     permtdist = gemaxclusterdist$astruct
   }

  # permtdist is the dist t-values, 
  # so p value is proportion of values greater than observed t-value.
  pstr = sum(abs(permtdist) > tstr, na.rm = TRUE)/length(permtdist)
  if (pstr < 0.025){
  print(paste("Cluster",cl,"Obs.sum t",round(tstr,3),"PropDist > observed p-value",pstr))
  }
  means.df$permtestp[means.df$time %in% bins & means.df$HeadNoun == lan] = pstr
}

# now we update our plot
p = ggplot(means.df , aes( x = time, y = target, linetype=SentenceType))
p = p + facet_wrap(~ HeadNoun, ncol=1)
# this pulls out the clusters which are significant by the permutation test
meansigStr = subset(means.df,permtestp < 0.025) 
# color them grey
if (length(meansigStr$time) > 0){
  p = p + geom_rect(data=meansigStr,aes(xmin=time-wsize-4, xmax=time+wsize+4, ymin = pliney+0.1, ymax= 1.0),colour=NA,fill="grey90",show.legend=FALSE)
}
# same as before
p = p + geom_line()
p = p + scale_linetype_discrete(name="RC Type")
p = p + theme_bw()   # change background to white
p = p + ylab("Target Proportion")
p = p + xlab("Time (msecs)")
p = p + geom_rect(aes(xmin=time-wsize, xmax=time+wsize, ymin = pliney, ymax= pline, colour=plinecol,fill=plinecol),show.legend=FALSE)
p = p + geom_curve(data=meansigPost,aes(x=time+10, xend=time+winsize-10, y = 0.9, yend= 0.9),color="black",size=1, lineend = "square", curvature = -0.5, show.legend=FALSE)
p = p + geom_rect(data=onsetStim,aes(xmin=time,xmax = time2-33, ymin=ypos-0.13, ymax=ypos,linetype=SentenceType),fill=NA,colour="black", show.legend=FALSE)
p = p + geom_text(data=onsetStim,aes(x=(time+time2)/2, y=ypos-0.05, label=cat), hjust=0.7, size=3,show.legend=FALSE)
p = p +scale_colour_grey()
p = p +scale_fill_grey()
apaformat(p)
ggsave("permCan.png",width=6,height=6)
```

The paper includes some figures to explain the permutation analysis.  Here is the figure that shows three permutations of the original data.

```{r,eval=TRUE,echo=FALSE,fig.height=3}
generateExampleExp <- function(){
  t2 = means.df$time[which(means.df$pstr == min(means.df$pstr,na.rm=TRUE))[1]]
#  print(t2)
  onetime = subset(pdata,HeadNoun == "ge3" & time == t2)
  randSet = onetime
  randmodel = lm(target ~ cobject ,randSet)
  #print(summary(randmodel))
  meantar = aggregate(target ~ cobject,onetime, mean)
  meantar = cbind(meantar,predict(randmodel, meantar, interval="confidence") )
  meantar$diff = meantar$fit[1]-meantar$fit[2]
  head(as.character(randSet$SentenceType),14)
  meantar$sim = "Observed"
  randSet$cobject = sample(randSet$cobject,length(randSet$cobject))
  randmodel = lm(target ~ cobject ,randSet)
  #print(summary(randmodel))
  meantar2 = aggregate(target ~ cobject,onetime, mean)
  meantar2 = cbind(meantar2,predict(randmodel, meantar2, interval="confidence") )
  meantar2$diff = meantar2$fit[1]-meantar2$fit[2]
  head(as.character(randSet$SentenceType),14)
  meantar2$sim = "Exp. 1"
  alldata = rbind(meantar,meantar2)
  randSet$cobject = sample(randSet$cobject,length(randSet$cobject))
  randmodel = lm(target ~ cobject ,randSet)
  #print(summary(randmodel))
  meantar3 = aggregate(target ~ cobject,onetime, mean)
  meantar3 = cbind(meantar3,predict(randmodel, meantar3, interval="confidence") )
  meantar3$diff = meantar3$fit[1]-meantar3$fit[2]
  head(as.character(randSet$SentenceType),14)
  meantar3$sim = "Exp. 2"
  alldata = rbind(alldata,meantar3)
  randSet$cobject = sample(randSet$cobject,length(randSet$cobject))
  randmodel = lm(target ~ cobject ,randSet)
  #print(summary(randmodel))
  meantar4 = aggregate(target ~ cobject,onetime, mean)
  meantar4 = cbind(meantar4,predict(randmodel, meantar4, interval="confidence") )
  meantar4$diff = meantar4$fit[1]-meantar4$fit[2]
  head(as.character(randSet$SentenceType),14)
  meantar4$sim = "Exp. 3"
  alldata = rbind(alldata,meantar4)
  return(alldata)
}

findData <- function(){
  notfound = 0
  while(notfound < 3){
    alldata <- generateExampleExp()
    notfound = 0
    if (alldata$diff[alldata$cobject==0.5 & alldata$sim == "Exp. 1"] < -0.2){
      notfound = notfound + 1
    }
    if (alldata$diff[alldata$cobject==0.5 & alldata$sim == "Exp. 2"] < 0.1){
      notfound = notfound + 1
    }
    if (alldata$diff[alldata$cobject==0.5 & alldata$sim == "Exp. 3"] > 0.2){
      notfound = notfound + 1
    }
  }
  alldata$cobject=factor(alldata$cobject,labels=c("Object","Subject"))
  return(alldata)
}
#alldata <- findData()
#write.csv(alldata,"exampleExp.csv")
alldata <- read.csv("exampleExp.csv")
alldata$cobject=factor(alldata$cobject,labels=c("Subject","Object"))
alldata$sim = factor(alldata$sim,levels=c("Observed","Exp. 1","Exp. 2","Exp. 3"))
p = ggplot(alldata,aes(x=cobject, y = target,ymin=lwr,ymax=upr))+geom_errorbar(width=0.25)+facet_wrap(~ sim, nrow=1)+ylab("Target proportion")+xlab("RC Type")
apaformat(p)
ggsave("ci.png",width=6,height=3)
```

Here is the accuracy data analysis

```{r,fig.height=3}
accdf = read.csv("accuracy data-19 for CL & 18 for GE - all correct trials.csv")
accdf$structure = factor(accdf$structure,labels=c("CL","ge3"))
accdf$extraction = factor(accdf$extraction ,labels=c("Subject","Object"),levels=c("subject","object"))

accdf$cCL = ifelse(accdf$structure == "CL",0.5,-0.5)
accdf$cobject = ifelse(accdf$extraction == "Object",0.5,-0.5)
accdf$participant=factor(accdf$participant)
accdf$item=factor(accdf$item)
xtabs( ~ structure + participant,accdf)
xtabs( ~ extraction + participant,accdf)
xtabs( ~ structure + item,accdf)
xtabs( ~ extraction + item,accdf)

acc.glm = glmer(Correct ~ cobject*cCL + (1 | participant) + (1  | item),accdf,family="binomial")
print(summary(acc.glm))

accdf$pred = remef(acc.glm, ran = "all")
head(accdf)
meandf = aggregate(Correct ~ extraction + structure, accdf, mean)
print(meandf)
# compute sd from pred without random effects
meandf$sd = aggregate(pred ~ extraction + structure, accdf,sd)$pred
print(meandf)
# se is computed from sd divided by number of participants
meandf$se = meandf$sd/sqrt( nlevels(accdf$participant) )
meandf$upper = meandf$Correct + meandf$se
meandf$lower = meandf$Correct - meandf$se
print(meandf)

p = ggplot(meandf, aes(x=structure,y=Correct, fill=extraction,ymin = lower, ymax=upper))
p = p +geom_bar(stat="identity",position="dodge")
p = p +scale_fill_grey(name="RC Type")
p = p + xlab("Sentence Type")
p = p + geom_errorbar(width=0.25, position=position_dodge(.9) )
apaformat(p)
ggsave("accuracy.png",width=5,height=3)

```